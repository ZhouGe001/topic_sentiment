{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'source', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'source', 'label'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'source', 'label'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", 'chinese')\n",
    "dataset\n",
    "# train_dataset = dataset[\"train\"]\n",
    "# sub_train_dataset = train_dataset.select(random.sample(range(len(train_dataset)), 12000))\n",
    "\n",
    "# # 现在sub_train_dataset就是包含12000条随机样本的数据集\n",
    "# print(sub_train_dataset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手动构建分类器层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig, BertTokenizerFast\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 加载预训练的BERT模型配置和模型本身\n",
    "config = BertConfig.from_pretrained('bert-base-chinese', num_labels=3)\n",
    "model = BertModel.from_pretrained('bert-base-chinese', config=config)\n",
    "\n",
    "# 添加一个分类层\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # 取CLS token的输出作为句子的表示\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 创建分类器实例\n",
    "classifier = BertClassifier(model, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# # 获取训练数据集\n",
    "# train_dataset = tokenized_datasets['train']\n",
    "# 获取测试数据集\n",
    "test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sub_train_dataset.map(tokenize_function, batched=True)\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练参数\n",
    "learning_rate = 2e-5\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "logging_steps = 1000\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# 选择优化器\n",
    "optimizer = AdamW(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# 设置损失函数\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# 设置学习率调度器\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "\n",
    "        optimizer.zero_grad()  # 清零梯度\n",
    "        outputs = classifier(input_ids, attention_mask=attention_mask)  # 前向传播\n",
    "        loss = criterion(outputs, labels)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新权重\n",
    "        scheduler.step()  # 更新学习率\n",
    "        \n",
    "        # 打印日志\n",
    "        if (step + 1) % logging_steps == 0:\n",
    "            print(f\"Step [{step+1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "    classifier.eval()  # 切换到评估模式\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['label']\n",
    "\n",
    "            outputs = classifier(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(test_dataset)\n",
    "    print(f'Epoch {epoch+1}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动构建分类器层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若本地有训练好的模型，则可以直接加载\n",
    "model_path = './三分类model'\n",
    "# 加载分词器\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "# 加载模型\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# # 加载BERT的tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# # 定义模型，3分类问题所以num_labels=3\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=3)\n",
    "\n",
    "# 定义tokenizer的处理函数\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    对输入的文本数据进行分词处理。\n",
    "\n",
    "    这个函数使用了预训练的分词器来将文本数据转换为模型输入所需的格式。\n",
    "    它通过对文本进行填充和截断，确保所有输入数据具有相同的长度，从而可以被模型有效地处理。\n",
    "\n",
    "    参数:\n",
    "    examples (dict): 包含文本数据的字典。文本数据存储在键为'text'的项中。\n",
    "\n",
    "    返回:\n",
    "    分词后的数据，以特定格式准备好供深度学习模型使用。\n",
    "    \"\"\"\n",
    "    # 使用tokenizer对文本数据进行分词、填充和截断\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# 使用map函数对dataset进行转换，将其中的文本数据通过tokenize_function进行分词处理\n",
    "# 这里的batched=True表示以批量方式处理数据，根据内存使用情况自动优化\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 设置数据集的格式为PyTorch兼容的格式，并指定包含的列\n",
    "# 这里的'input_ids'和'attention_mask'是模型输入所需的列，'label'是标签列\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# 获取训练数据集\n",
    "train_dataset = tokenized_datasets['train']\n",
    "# 获取测试数据集\n",
    "test_dataset = tokenized_datasets['test']\n",
    "\n",
    "# 定义一个计算模型性能指标的函数\n",
    "# 它接受一个预测对象p作为输入，该对象应包含实际标签和预测标签的信息\n",
    "# 它返回一个字典，其中包含了准确率、精确率、召回率和F1分数四个指标的值\n",
    "def compute_metrics(p):\n",
    "    # 从预测对象中获取预测结果，将其转换为最大概率对应的类别标签\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    \n",
    "    # 计算预测结果的精确率、召回率、F1分数和支持度，这里使用的是宏平均方法\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='macro')\n",
    "    \n",
    "    # 计算预测结果的准确率\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    \n",
    "    # 将计算得到的指标封装在一个字典中并返回\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# 定义训练参数\n",
    "# 这段代码用于配置模型训练时的参数，为`TrainingArguments`类的实例。\n",
    "# `output_dir`指定训练结果的输出目录。\n",
    "# `evaluation_strategy`定义评估模型的策略，这里为每个epoch进行一次评估。\n",
    "# `learning_rate`设置初始学习率，控制模型参数更新的速度。\n",
    "# `per_device_train_batch_size`和`per_device_eval_batch_size`分别定义了训练和评估时每个设备的批次大小。\n",
    "# `num_train_epochs`指定训练的轮数，这里为4轮。\n",
    "# `weight_decay`设置权重衰减（L2正则化）的系数，用于防止过拟合。\n",
    "# `logging_dir`定义日志存储目录，用于记录训练过程中的信息。\n",
    "# `logging_steps`指定每隔多少步打印一次日志。\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results-三分类',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,\n",
    ")\n",
    "\n",
    "# 创建一个带有填充功能的数据合并器\n",
    "# 这个数据合并器会在训练过程中对输入数据进行预处理，具体包括：\n",
    "# - 使用传入的 tokenizer 对文本数据进行编码，将其转换为模型可接受的格式\n",
    "# - 对不同长度的序列进行统一长度的调整，通过在较短序列的末尾添加特殊填充标记来实现\n",
    "# - 保证批次内的所有序列长度一致，便于模型的批量处理\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 创建Trainer实例\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save_pretrained('./三分类model')\n",
    "tokenizer.save_pretrained('./三分类model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义推理函数\n",
    "def predict(text):\n",
    "    \"\"\"\n",
    "    定义推理函数，用于对输入文本进行情感预测。\n",
    "    \n",
    "    Args:\n",
    "        text (str): 输入文本。\n",
    "        \n",
    "    Returns:\n",
    "        str: 预测结果，可以是\"Positive\"、\"Negative\"或\"Neutral\"。\n",
    "    \"\"\"\n",
    "    # 确定设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # 将模型移动到相同的设备\n",
    "    model.to(device)\n",
    "    # 关闭模型的训练模式\n",
    "    model.eval()\n",
    "    # 预处理输入文本\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    # 将输入数据移动到相同的设备\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    # 关闭梯度计算以节省内存和计算力\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # 获取预测结果\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    # 初始化结果变量\n",
    "    result=''\n",
    "    if predictions.item() == 0:\n",
    "        result = \"Positive\" \n",
    "    elif predictions.item() == 2:\n",
    "        result = \"Negative\"\n",
    "    else:\n",
    "        result = \"Neutral\"\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例推理\n",
    "example_text = \"ChatGPT不会回答这个问题\"\n",
    "print(predict(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取xlsx文件并去重\n",
    "df = pd.read_excel('ChatGPT_去重.xlsx')  # 替换为你的xlsx文件路径\n",
    "def check_length(row):\n",
    "    return len(row['微博博文']) < 200\n",
    "\n",
    "filtered_df = df[df.apply(check_length, axis=1)]\n",
    "\n",
    "# 假设文本内容在\"微博博文\"列\n",
    "texts = filtered_df['微博博文'].tolist()\n",
    "\n",
    "# 创建一个进度条\n",
    "tqdm.pandas()\n",
    "\n",
    "# 对每个文本进行情感预测\n",
    "df['sentiment'] = df['微博博文'].progress_apply(predict)\n",
    "\n",
    "# 保存结果回xlsx文件\n",
    "df.to_excel('predicted_sentiments.xlsx', index=False)\n",
    "\n",
    "print(\"情感预测完成，并已保存到predicted_sentiments.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
